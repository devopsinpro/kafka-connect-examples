# GCS Sink example for Kafka Connect
# Two examples are provided and this example is meant to
# be accompanied with blog post and screencast
# at https://supergloo.com
# 
# You need to change `gcs.bucket.name` and `gcs.credentials` vars
# and set either AVRO or JSON outputs depending on your source 
# data.  Default is Avro using the orders example
#
# You may need to change other settings if you go outside of the
# quickstart demos I provided.  
# Good luck, we're all counting on you
# 

name=gcs-sink
connector.class=io.confluent.connect.gcs.GcsSinkConnector
tasks.max=1

time.interval=HOURLY

gcs.bucket.name=kafka-connect-example
gcs.part.size=5242880
flush.size=3

gcs.credentials.path=/<your-path-to>/gcp-key.json
storage.class=io.confluent.connect.gcs.storage.GcsStorage

# Key converter same for both examples
key.converter=org.apache.kafka.connect.storage.StringConverter

# Avro example with orders test data
topics=orders
format.class=io.confluent.connect.gcs.format.avro.AvroFormat
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081

# JSON example with pageviews test data
# topics=pageviews
# format.class=io.confluent.connect.gcs.format.json.JsonFormat
# value.converter=org.apache.kafka.connect.json.JsonConverter
# value.converter.schemas.enable=false

schema.compatibility=NONE

partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner

# Example values when using a time-based partitioner
#partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
# For instance, set partition duration to 1 hour
#partition.duration.ms=3600000
#path.format='year'=YYYY_'month'=MM_'day'=dd_'hour'=HH
#locale=en
#timezone=America/Los_Angeles
# Use a deterministic partitioner based on Kafka record timestamps
#timestamp.extractor=Record
# For example, rotate a file every 15 minutes
#rotate.interval.ms=900000
# Also, set size-based rotation to a high value
#flush.size=1000000

# Or use a field based partitioner
#partitioner.class=io.confluent.connect.storage.partitioner.FieldPartitioner
#partition.field.name=


#############################
# CONFLUENT LICENSE SETTINGS
#############################

# Confluent will issue a license key to each subscriber. The license key will be a short snippet
# of text that you can copy and paste. Without the license key, you can use this connector for a
# 30-day trial period. If you are a subscriber, please contact Confluent Support for more
# information.
#confluent.license=

# Name of the Kafka topic used for Confluent Platform configuration, including licensing
# information.
#confluent.topic=_confluent-command

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster
# used for licensing. All servers in the cluster will be discovered from the initial connection.
# This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these servers
# are just used for the initial connection to discover the full cluster membership (which may
# change dynamically), this list need not contain the full set of servers (you may want more than
# one, though, in case a server is down).
# This is a required config property
confluent.topic.bootstrap.servers=localhost:9092

# The replication factor for the Kafka topic used for Confluent Platform configuration, including
# licensing information. This is used only if the topic does not already exist, and the default
# of 3 is appropriate for production use. If you are using a development environment with less
# than 3 brokers, you must set this to the number of brokers (often 1).
#confluent.topic.replication.factor=3
confluent.topic.replication.factor=1

# In secured clusters, add any security configs to the Kafka clients used on confluent.topic by 
# using the prefixes, such as confluent.topic. 
# or confluent.topic.producer. and confluent.topic.consumer. 
# if settings differ between the producer and the consumer that read the confluent.topic
